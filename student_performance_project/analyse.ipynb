{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043b86d0",
   "metadata": {},
   "source": [
    "# Student Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86600558",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "Here, we import the essential libraries for our analysis:\n",
    "- **pandas (`pd`):** For data manipulation and reading CSV files.\n",
    "- **numpy (`np`):** For numerical operations.\n",
    "- **matplotlib.pyplot (`plt`):** For creating basic plots.\n",
    "- **seaborn (`sns`):** For creating more attractive and informative statistical plots.\n",
    "\n",
    "We also set the default plot style to `darkgrid` to make our visualizations easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95b529ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\'darkgrid\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14502586",
   "metadata": {},
   "source": [
    "### 2. Loading the Data\n",
    "\n",
    "We load the `StudentsPerformance.csv` dataset into a pandas DataFrame called `df`. This is the first step in any data analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6640448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\'/home/user/veghar/student_performance_project/StudentsPerformance.csv\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbc3a1",
   "metadata": {},
   "source": [
    "### 3. Initial Data Exploration\n",
    "\n",
    "Let\'s start by getting a feel for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3004c7",
   "metadata": {},
   "source": [
    "#### Displaying the First 5 Rows\n",
    "\n",
    "Using `df.head()`, we can see the first few rows of the dataset. This helps us understand the column names and the type of data in each column (e.g., categorical or numerical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e53ebbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70080894",
   "metadata": {},
   "source": [
    "#### Displaying the Last 5 Rows\n",
    "\n",
    "`df.tail()` shows us the end of the dataset, which can sometimes help spot any issues with the data import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8cbb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ce1ab",
   "metadata": {},
   "source": [
    "#### Statistical Summary\n",
    "\n",
    "`df.describe()` provides a statistical summary for the numerical columns.\n",
    "\n",
    "**Observations:**\n",
    "- The `count` for all score columns is 1000, indicating no missing values.\n",
    "- The `mean` (average) scores are around 66-69.\n",
    "- The `min` score in math is 0, which might be an outlier or a point of interest. Reading and writing scores have higher minimums (17 and 10, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4755bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd3dbd",
   "metadata": {},
   "source": [
    "#### Data Types and Non-Null Counts\n",
    "\n",
    "`df.info()` gives us a concise summary of the DataFrame.\n",
    "\n",
    "**Observations:**\n",
    "- There are 1000 entries (rows).\n",
    "- All columns have 1000 non-null values, confirming there is no missing data to handle.\n",
    "- We have 5 `object` (categorical) columns and 3 `int64` (numerical) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9d2e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28b016",
   "metadata": {},
   "source": [
    "#### Checking the Shape of the Data\n",
    "\n",
    "`df.shape` returns a tuple representing the dimensionality of the DataFrame (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f196a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778bb4d",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values\n",
    "\n",
    "This command confirms our earlier observation from `df.info()` that there are no missing (`NaN`) values in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "860c0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d4c9f",
   "metadata": {},
   "source": [
    "#### Checking for Duplicate Rows\n",
    "\n",
    "`df.duplicated().sum()` counts the number of duplicate rows. A result of 0 is good, as it means every student record is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75205142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83652057",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering\n",
    "\n",
    "Now, we will create new columns (features) from the existing data to make our analysis more insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36869096",
   "metadata": {},
   "source": [
    "#### Calculating Total and Average Scores\n",
    "\n",
    "We create a `total score` by summing the three subject scores and an `average score` from that total. We then round the average score and create a new `avrage score` column for simplicity. Finally, we drop the unrounded `average score` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6be58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\'total score\'] = df[\'math score\'] + df[\'reading score\'] + df[\'writing score\']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-3",
   "metadata": {},
   "source": [
    "This calculates the average score by dividing the total score by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07028e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\'average score\'] = df[\'total score\'] / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-4",
   "metadata": {},
   "source": [
    "This rounds the average score to the nearest whole number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1244e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\'avrage score\'] =  df[\'average score\'].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-5",
   "metadata": {},
   "source": [
    "This drops the original, unrounded average score column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7c06a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\'average score\'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36615cd7",
   "metadata": {},
   "source": [
    "Let\'s check the DataFrame again to see our new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbc79815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a680d",
   "metadata": {},
   "source": [
    "#### Creating a Performance Category\n",
    "\n",
    "To make the performance easier to analyze, we\'ll categorize the rounded average score into three levels: `High`, `Medium`, and `Low`.\n",
    "We define a function `performace_score` that assigns these categories and then use `.apply()` to create the new `performance score` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0fdab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performace_score(avg) -> int:\n",
    "    if avg > 75:\n",
    "        return \'High\'\n",
    "    elif avg >= 50:\n",
    "        return \'Medium\'\n",
    "    else:\n",
    "        return \'Low\'\n",
    "\n",
    "df[\'performance score\'] = df[\'avrage score\'].apply(performace_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db51282",
   "metadata": {},
   "source": [
    "#### Checking the Distribution of Performance Scores\n",
    "\n",
    "**Observation:**\n",
    "Most students (594) fall into the `Medium` performance category. 306 are `High` performers, and 100 are `Low` performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a41d649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\'performance score\'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50652a59",
   "metadata": {},
   "source": [
    "### 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let\'s dive deeper into the data and uncover relationships between different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed64b0",
   "metadata": {},
   "source": [
    "#### Top 5 Students by Average Score\n",
    "\n",
    "Here we sort the DataFrame by the `avrage score` in descending order to see the top-performing students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93eb9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"avrage score\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bbe84",
   "metadata": {},
   "source": [
    "#### Gender Distribution\n",
    "\n",
    "**Observation:**\n",
    "The dataset is fairly balanced by gender, with slightly more females (518) than males (482)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "926d7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\'gender\')[\'performance score\'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c26a29",
   "metadata": {},
   "source": [
    "#### Average Scores by Gender\n",
    "\n",
    "**Observation:**\n",
    "- **Females** tend to have higher average scores in `reading` and `writing`.\n",
    "- **Males** tend to have a higher average score in `math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "537aabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\'gender\')[[\'math score\',\'reading score\',\'writing score\']].mean().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62d7bd",
   "metadata": {},
   "source": [
    "#### Performance Distribution by Gender\n",
    "\n",
    "**Observation:**\n",
    "Females have a higher count of `High` performers (180 vs. 126) and a lower count of `Low` performers (38 vs. 62) compared to males. This aligns with the previous finding that females have higher average reading and writing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1095842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\'gender\')[\'performance score\'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a15a5e",
   "metadata": {},
   "source": [
    "#### Impact of Test Preparation Course\n",
    "\n",
    "Let\'s see how many students completed the test preparation course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79e70059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\'gender\')[\'test preparation course\'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa70245",
   "metadata": {},
   "source": [
    "And now let\'s see the impact on the average score.\n",
    "\n",
    "**Observation:**\n",
    "This is a significant finding. Students who **completed** the test preparation course have an average score of **73**, while those who did not have an average score of **65**. This is a clear indicator of the course\'s effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b89d51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\'test preparation course\')[\'avrage score\'].mean().reset_index().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3faef",
   "metadata": {},
   "source": [
    "### 6. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c8925",
   "metadata": {},
   "source": [
    "#### Visualization 1: Performance Score by Test Preparation\n",
    "\n",
    "This plot visualizes the observation from the previous step. We use `sns.countplot` with `hue` to compare the performance scores of students who completed the course versus those who did not.\n",
    "\n",
    "**Observation from Plot:**\n",
    "The blue bars (`none`) are concentrated in the `Medium` and `Low` categories, while the orange bars (`completed`) are much more prominent in the `High` performance category. This visually confirms that completing the prep course is strongly associated with higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d0b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\'performance score\', hue=\'test preparation course\')\n",
    "plt.title(\'Performance Score by Test Preparation\')\n",
    "plt.xlabel(\'Performance Score\')\n",
    "plt.ylabel(\'Number of Students\')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b78e0",
   "metadata": {},
   "source": [
    "#### Visualization 2: Performance by Race/Ethnicity\n",
    "\n",
    "This plot shows the distribution of performance scores across different racial/ethnic groups.\n",
    "\n",
    "**Observation from Plot:**\n",
    "- **Group E** has the highest proportion of `High` performers.\n",
    "- **Group A** and **Group B** have a higher proportion of `Low` and `Medium` performers compared to the other groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6382bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.countplot(data=df,x=\'race/ethnicity\',hue=\'performance score\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034548c",
   "metadata": {},
   "source": [
    "#### Visualization 3: Performance by Gender\n",
    "\n",
    "This plot shows the distribution of performance scores across gender.\n",
    "\n",
    "**Observation from Plot:**\n",
    "As noted earlier, a higher number of `High` performers are female, while a higher number of `Low` performers are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cee53da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\'darkgrid\')\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.countplot(data=df,x=\'performance score\',hue=\'gender\',width=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-6",
   "metadata": {},
   "source": [
    "This shows the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61238622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-7",
   "metadata": {},
   "source": [
    "This shows the value counts of the parental level of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f494c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\'parental level of education\'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-8",
   "metadata": {},
   "source": [
    "This chart shows the relationship between parental level of education and whether the student completed the test preparation course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0389a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.countplot(data=df,x=\'parental level of education\',hue=\'test preparation course\')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-9",
   "metadata": {},
   "source": [
    "This shows the first 5 rows of the dataframe, which now includes the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45f48bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-10",
   "metadata": {},
   "source": [
    "This creates a pie chart of the performance score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e223f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_chart = df[\'performance score\'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaab06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "explode_values = [0.1 if x==pie_chart.max() else 0 for x in pie_chart]\n",
    "plt.pie(pie_chart,labels=pie_chart.index,autopct=\'%1.1f%%\',startangle=90,shadow=True,explode=explode_values)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-11",
   "metadata": {},
   "source": [
    "This shows all the available functions and attributes in the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "684a0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-1",
   "metadata": {},
   "source": [
    "What does this chart show?\n",
    "\n",
    "Why does this matter?\n",
    "\n",
    "Example style (sirf example, copy nahi):\n",
    "\n",
    "“This chart shows that …”\n",
    "\n",
    "“This suggests that …”\n",
    "\n",
    "⚠️ Rules:\n",
    "\n",
    "No judgement\n",
    "\n",
    "No assumptions\n",
    "\n",
    "Sirf observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-2",
   "metadata": {},
   "source": [
    "### Strong Insight: The Impact of the Test Preparation Course\n",
    "\n",
    "**Observation (Kya observe kiya):**\n",
    "The data clearly shows that students who completed the test preparation course performed significantly better than those who did not. The average score for students who completed the course is 73, while the average score for those who did not is 65. This is further supported by the \"Performance Score by Test Preparation\" visualization, which shows a much higher concentration of \"High\" performers among the students who completed the course.\n",
    "\n",
    "**Importance (Kyu important hai):**\n",
    "This observation is important because it provides a clear, actionable insight. The 8-point difference in the average score is substantial and can mean the difference between a student being categorized as a \"Medium\" performer versus a \"High\" performer. This suggests that the test preparation course is a highly effective intervention. Educational institutions can leverage this data to make informed decisions, such as encouraging or even requiring the test preparation course to help students improve their academic outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
